{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:39:28.051329300Z",
     "start_time": "2024-06-15T11:39:28.020846200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.getcwd()\n",
    "train_dir = os.path.join(base_dir, 'dataset/train')\n",
    "val_dir = os.path.join(base_dir, 'dataset/valid')\n",
    "test_dir = os.path.join(base_dir, 'dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:39:34.293174400Z",
     "start_time": "2024-06-15T11:39:28.039148900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2087 images belonging to 2 classes.\n",
      "Found 334 images belonging to 2 classes.\n",
      "Found 268 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Defining constants\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Loading dataset using ImageDataGenerator function\n",
    "datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:39:39.024762800Z",
     "start_time": "2024-06-15T11:39:34.245291700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722596955.454706    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.541931    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.542000    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.545334    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.545408    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.545434    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.889533    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.889616    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722596955.889665    4541 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenet_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenet_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m3,228,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,888,321</span> (14.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,888,321\u001b[0m (14.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,864,897</span> (14.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,864,897\u001b[0m (14.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,424</span> (91.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,424\u001b[0m (91.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the MobileNet model with pre-trained weights, excluding the top layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
    "\n",
    "# Unfreezing some of the last layers of the base model\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Creating input tensor\n",
    "inputs = Input(shape=(img_size[0], img_size[1], 3))\n",
    "\n",
    "# Passing inputs through the base model\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Adding custom layers on top of the base model\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compiling the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:41:22.892130400Z",
     "start_time": "2024-06-15T11:41:22.863643Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_path = os.path.join(base_dir, 'sleep_detection_model.keras')\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:47:47.340401400Z",
     "start_time": "2024-06-15T11:41:26.487561Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosmos/miniconda3/envs/DataScienceGPU/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722597119.396923    4682 service.cc:146] XLA service 0x7f83f8004620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722597119.397041    4682 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "I0000 00:00:1722597146.142216    4682 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.5452 - loss: 0.9786\n",
      "Epoch 1: val_accuracy improved from -inf to 0.65569, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 1s/step - accuracy: 0.5455 - loss: 0.9776 - val_accuracy: 0.6557 - val_loss: 0.6147\n",
      "Epoch 2/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.6385 - loss: 0.7997\n",
      "Epoch 2: val_accuracy improved from 0.65569 to 0.73952, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 277ms/step - accuracy: 0.6385 - loss: 0.7997 - val_accuracy: 0.7395 - val_loss: 0.5446\n",
      "Epoch 3/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.6559 - loss: 0.7466\n",
      "Epoch 3: val_accuracy improved from 0.73952 to 0.78443, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 272ms/step - accuracy: 0.6560 - loss: 0.7464 - val_accuracy: 0.7844 - val_loss: 0.4854\n",
      "Epoch 4/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6871 - loss: 0.6750\n",
      "Epoch 4: val_accuracy improved from 0.78443 to 0.80838, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 276ms/step - accuracy: 0.6871 - loss: 0.6748 - val_accuracy: 0.8084 - val_loss: 0.4716\n",
      "Epoch 5/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.7214 - loss: 0.5967\n",
      "Epoch 5: val_accuracy improved from 0.80838 to 0.81138, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 282ms/step - accuracy: 0.7215 - loss: 0.5968 - val_accuracy: 0.8114 - val_loss: 0.4647\n",
      "Epoch 6/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7191 - loss: 0.6231\n",
      "Epoch 6: val_accuracy improved from 0.81138 to 0.81437, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 274ms/step - accuracy: 0.7192 - loss: 0.6229 - val_accuracy: 0.8144 - val_loss: 0.4488\n",
      "Epoch 7/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.7423 - loss: 0.6080\n",
      "Epoch 7: val_accuracy improved from 0.81437 to 0.82934, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 280ms/step - accuracy: 0.7425 - loss: 0.6071 - val_accuracy: 0.8293 - val_loss: 0.4239\n",
      "Epoch 8/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.7518 - loss: 0.5614\n",
      "Epoch 8: val_accuracy improved from 0.82934 to 0.83832, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 299ms/step - accuracy: 0.7519 - loss: 0.5613 - val_accuracy: 0.8383 - val_loss: 0.4017\n",
      "Epoch 9/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.7757 - loss: 0.5092\n",
      "Epoch 9: val_accuracy improved from 0.83832 to 0.84731, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 276ms/step - accuracy: 0.7756 - loss: 0.5097 - val_accuracy: 0.8473 - val_loss: 0.3897\n",
      "Epoch 10/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.7850 - loss: 0.4948\n",
      "Epoch 10: val_accuracy improved from 0.84731 to 0.85329, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 282ms/step - accuracy: 0.7850 - loss: 0.4946 - val_accuracy: 0.8533 - val_loss: 0.3785\n",
      "Epoch 11/20\n",
      "\u001b[1m65/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8062 - loss: 0.4602\n",
      "Epoch 11: val_accuracy improved from 0.85329 to 0.85928, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 282ms/step - accuracy: 0.8061 - loss: 0.4602 - val_accuracy: 0.8593 - val_loss: 0.3692\n",
      "Epoch 12/20\n",
      "\u001b[1m65/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.7949 - loss: 0.4587\n",
      "Epoch 12: val_accuracy improved from 0.85928 to 0.88024, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 284ms/step - accuracy: 0.7953 - loss: 0.4581 - val_accuracy: 0.8802 - val_loss: 0.3335\n",
      "Epoch 13/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8118 - loss: 0.4381\n",
      "Epoch 13: val_accuracy improved from 0.88024 to 0.88623, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 290ms/step - accuracy: 0.8119 - loss: 0.4380 - val_accuracy: 0.8862 - val_loss: 0.3159\n",
      "Epoch 14/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8387 - loss: 0.4053\n",
      "Epoch 14: val_accuracy improved from 0.88623 to 0.88922, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 270ms/step - accuracy: 0.8384 - loss: 0.4054 - val_accuracy: 0.8892 - val_loss: 0.3203\n",
      "Epoch 15/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8295 - loss: 0.4121\n",
      "Epoch 15: val_accuracy did not improve from 0.88922\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 260ms/step - accuracy: 0.8297 - loss: 0.4117 - val_accuracy: 0.8892 - val_loss: 0.3052\n",
      "Epoch 16/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8409 - loss: 0.3677\n",
      "Epoch 16: val_accuracy improved from 0.88922 to 0.90120, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 259ms/step - accuracy: 0.8409 - loss: 0.3677 - val_accuracy: 0.9012 - val_loss: 0.2923\n",
      "Epoch 17/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8528 - loss: 0.3879\n",
      "Epoch 17: val_accuracy improved from 0.90120 to 0.91018, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 280ms/step - accuracy: 0.8527 - loss: 0.3877 - val_accuracy: 0.9102 - val_loss: 0.2762\n",
      "Epoch 18/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8708 - loss: 0.3250\n",
      "Epoch 18: val_accuracy did not improve from 0.91018\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 264ms/step - accuracy: 0.8708 - loss: 0.3253 - val_accuracy: 0.9102 - val_loss: 0.2816\n",
      "Epoch 19/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8857 - loss: 0.3195\n",
      "Epoch 19: val_accuracy improved from 0.91018 to 0.92515, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 264ms/step - accuracy: 0.8856 - loss: 0.3195 - val_accuracy: 0.9251 - val_loss: 0.2580\n",
      "Epoch 20/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8724 - loss: 0.3220\n",
      "Epoch 20: val_accuracy improved from 0.92515 to 0.93114, saving model to /mnt/d/NullClass_Internship/Sleep_Detection/sleep_detection_model.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 276ms/step - accuracy: 0.8724 - loss: 0.3218 - val_accuracy: 0.9311 - val_loss: 0.2494\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the model\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:16:10.922680600Z",
     "start_time": "2024-06-15T07:15:59.001756400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "import gradio as gr\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "saved_model_dir = os.path.join(base_dir, 'saved_model')\n",
    "\n",
    "# Loading the trained CNN model\n",
    "model = load_model(saved_model_dir)\n",
    "\n",
    "# Initializing the MTCNN face detector\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T22:43:57.772018Z",
     "start_time": "2024-06-14T22:43:57.767019500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making a function for fetching roi coordinates, performing classification and displaying image having detection\n",
    "def classify_faces(img):\n",
    "    faces = detector.detect_faces(img)\n",
    "    sleepy_faces = 0\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, w, h = face['box']\n",
    "        x1 = face['keypoints']['left_eye'][0]\n",
    "        y1 = face['keypoints']['left_eye'][1]\n",
    "        x2 = face['keypoints']['right_eye'][0]\n",
    "        y2 = face['keypoints']['right_eye'][1]\n",
    "\n",
    "        # Calculating the distance between the eyes\n",
    "        eye_distance = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "        if abs(x2 - x1) > abs(y2 - y1):\n",
    "            # For larger horizontal distances between eyes\n",
    "            roi_w = int(5 / 3 * eye_distance)\n",
    "            roi_h = int(2 / 3 * eye_distance)\n",
    "        else:\n",
    "            # For larger vertical distances between eyes\n",
    "            roi_w = int(2 / 3 * eye_distance)\n",
    "            roi_h = int(5 / 3 * eye_distance)\n",
    "\n",
    "        # Calculating the center point between the eyes\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "\n",
    "        # Adjusting ROI coordinates to keep the center point between the eyes (It essentially grabs the top left\n",
    "        # coordinate of the roi box)\n",
    "        roi_x = int(center_x - roi_w / 2)\n",
    "        roi_y = int(center_y - roi_h / 2)\n",
    "\n",
    "        # Ensuring the ROI is within image boundaries\n",
    "        roi_x = max(0, roi_x)\n",
    "        roi_y = max(0, roi_y)\n",
    "        roi_w = min(roi_w, img.shape[1] - roi_x)\n",
    "        roi_h = min(roi_h, img.shape[0] - roi_y)\n",
    "\n",
    "        crop = img[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n",
    "\n",
    "        # Preprocessing the cropped face image\n",
    "        crop_resized = cv2.resize(crop, (224, 224))\n",
    "        crop_resized = crop_resized.astype('float32') / 255.0\n",
    "        crop_resized = np.expand_dims(crop_resized, axis=0)\n",
    "\n",
    "        prediction = model.predict(crop_resized)\n",
    "        label = 'Awake' if prediction[0][0] < 0.5 else 'Sleepy'\n",
    "\n",
    "        if label == 'Sleepy':\n",
    "            sleepy_faces += 1\n",
    "            # Drawing bounding box around drowsy face\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            # Putting text label above the bounding box\n",
    "            cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    # Displaying the count of sleepy faces detected\n",
    "    cv2.putText(img, f'Sleepy faces: {sleepy_faces}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    return img, sleepy_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T22:43:58.634912400Z",
     "start_time": "2024-06-14T22:43:58.605126100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Unable to load image from {image_path}\")\n",
    "\n",
    "    # Converting BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resizing the image to fit within a fixed window size while maintaining aspect ratio\n",
    "    max_display_size = 800  # Maximum width or height for displaying the image\n",
    "    height, width, _ = img_rgb.shape\n",
    "    if max(height, width) > max_display_size:\n",
    "        if height > width:\n",
    "            new_height = max_display_size\n",
    "            new_width = int(width * (max_display_size / height))\n",
    "        else:\n",
    "            new_width = max_display_size\n",
    "            new_height = int(height * (max_display_size / width))\n",
    "        img_rgb = cv2.resize(img_rgb, (new_width, new_height))\n",
    "\n",
    "    # Classifying faces and retrieving image with bounding boxes\n",
    "    img_with_boxes, sleepy_faces = classify_faces(img_rgb)\n",
    "\n",
    "    # Converting back to BGR for saving with OpenCV\n",
    "    img_with_boxes_bgr = cv2.cvtColor(img_with_boxes, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return img_with_boxes_bgr, f'Sleepy faces detected: {sleepy_faces}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T22:43:59.451538200Z",
     "start_time": "2024-06-14T22:43:59.428539100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    max_sleepy_faces = 0\n",
    "\n",
    "    # Obtaining frame dimensions and FPS from the video capture\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Converting the frame from BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        frame_with_boxes, sleepy_faces = classify_faces(frame_rgb)\n",
    "        frames.append(frame_with_boxes)\n",
    "\n",
    "        # Updating maximum sleepy faces count if current frame has more\n",
    "        if sleepy_faces > max_sleepy_faces:\n",
    "            max_sleepy_faces = sleepy_faces\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Saving the processed video to a temporary file\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n",
    "    out = cv2.VideoWriter(temp_file.name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    for frame in frames:\n",
    "        # Converting the frame back to BGR for saving\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    return temp_file.name, f'The maximum number of sleepy faces detected in the entire video is: {max_sleepy_faces}'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
