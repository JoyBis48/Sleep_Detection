{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:39:28.051329300Z",
     "start_time": "2024-06-15T11:39:28.020846200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.getcwd()\n",
    "train_dir = os.path.join(base_dir, 'dataset/train')\n",
    "val_dir = os.path.join(base_dir, 'dataset/valid')\n",
    "test_dir = os.path.join(base_dir, 'dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:39:34.293174400Z",
     "start_time": "2024-06-15T11:39:28.039148900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2090 images belonging to 2 classes.\n",
      "Found 334 images belonging to 2 classes.\n",
      "Found 268 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Defining constants\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Loading dataset using ImageDataGenerator function\n",
    "datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:39:39.024762800Z",
     "start_time": "2024-06-15T11:39:34.245291700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,888,321\n",
      "Trainable params: 3,864,897\n",
      "Non-trainable params: 23,424\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading the MobileNet model with pre-trained weights, excluding the top layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
    "\n",
    "# Unfreezing some of the last layers of the base model\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Creating input tensor\n",
    "inputs = Input(shape=(img_size[0], img_size[1], 3))\n",
    "\n",
    "# Passing inputs through the base model\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Adding custom layers on top of the base model\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compiling the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:41:22.892130400Z",
     "start_time": "2024-06-15T11:41:22.863643Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_path = os.path.join(base_dir, 'Other_format/sleep_detection_model.h5')\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:47:47.340401400Z",
     "start_time": "2024-06-15T11:41:26.487561Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8427 - accuracy: 0.6105\n",
      "Epoch 1: val_accuracy improved from -inf to 0.89222, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 73s 970ms/step - loss: 0.8427 - accuracy: 0.6105 - val_loss: 0.3911 - val_accuracy: 0.8922\n",
      "Epoch 2/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6108 - accuracy: 0.7325\n",
      "Epoch 2: val_accuracy improved from 0.89222 to 0.93413, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 16s 248ms/step - loss: 0.6108 - accuracy: 0.7325 - val_loss: 0.2839 - val_accuracy: 0.9341\n",
      "Epoch 3/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.7679\n",
      "Epoch 3: val_accuracy did not improve from 0.93413\n",
      "66/66 [==============================] - 16s 236ms/step - loss: 0.5440 - accuracy: 0.7679 - val_loss: 0.3150 - val_accuracy: 0.8892\n",
      "Epoch 4/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.8057\n",
      "Epoch 4: val_accuracy improved from 0.93413 to 0.94012, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 17s 251ms/step - loss: 0.4608 - accuracy: 0.8057 - val_loss: 0.2554 - val_accuracy: 0.9401\n",
      "Epoch 5/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.8426\n",
      "Epoch 5: val_accuracy improved from 0.94012 to 0.95509, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 17s 253ms/step - loss: 0.3975 - accuracy: 0.8426 - val_loss: 0.1652 - val_accuracy: 0.9551\n",
      "Epoch 6/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.8536\n",
      "Epoch 6: val_accuracy did not improve from 0.95509\n",
      "66/66 [==============================] - 16s 237ms/step - loss: 0.3622 - accuracy: 0.8536 - val_loss: 0.1630 - val_accuracy: 0.9551\n",
      "Epoch 7/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8732\n",
      "Epoch 7: val_accuracy improved from 0.95509 to 0.96108, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 16s 248ms/step - loss: 0.3329 - accuracy: 0.8732 - val_loss: 0.1570 - val_accuracy: 0.9611\n",
      "Epoch 8/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.8789\n",
      "Epoch 8: val_accuracy did not improve from 0.96108\n",
      "66/66 [==============================] - 16s 237ms/step - loss: 0.3028 - accuracy: 0.8789 - val_loss: 0.1380 - val_accuracy: 0.9611\n",
      "Epoch 9/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9010\n",
      "Epoch 9: val_accuracy improved from 0.96108 to 0.96707, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 17s 254ms/step - loss: 0.2779 - accuracy: 0.9010 - val_loss: 0.1484 - val_accuracy: 0.9671\n",
      "Epoch 10/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9043\n",
      "Epoch 10: val_accuracy did not improve from 0.96707\n",
      "66/66 [==============================] - 16s 237ms/step - loss: 0.2683 - accuracy: 0.9043 - val_loss: 0.1775 - val_accuracy: 0.9581\n",
      "Epoch 11/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.9139\n",
      "Epoch 11: val_accuracy did not improve from 0.96707\n",
      "66/66 [==============================] - 16s 238ms/step - loss: 0.2563 - accuracy: 0.9139 - val_loss: 0.1244 - val_accuracy: 0.9671\n",
      "Epoch 12/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9239\n",
      "Epoch 12: val_accuracy improved from 0.96707 to 0.97006, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 17s 254ms/step - loss: 0.2417 - accuracy: 0.9239 - val_loss: 0.1436 - val_accuracy: 0.9701\n",
      "Epoch 13/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.9292\n",
      "Epoch 13: val_accuracy did not improve from 0.97006\n",
      "66/66 [==============================] - 16s 237ms/step - loss: 0.2101 - accuracy: 0.9292 - val_loss: 0.1576 - val_accuracy: 0.9701\n",
      "Epoch 14/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1988 - accuracy: 0.9349\n",
      "Epoch 14: val_accuracy improved from 0.97006 to 0.97605, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 16s 248ms/step - loss: 0.1988 - accuracy: 0.9349 - val_loss: 0.1204 - val_accuracy: 0.9760\n",
      "Epoch 15/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9383\n",
      "Epoch 15: val_accuracy did not improve from 0.97605\n",
      "66/66 [==============================] - 16s 238ms/step - loss: 0.1990 - accuracy: 0.9383 - val_loss: 0.1957 - val_accuracy: 0.9551\n",
      "Epoch 16/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9359\n",
      "Epoch 16: val_accuracy did not improve from 0.97605\n",
      "66/66 [==============================] - 16s 239ms/step - loss: 0.1926 - accuracy: 0.9359 - val_loss: 0.1261 - val_accuracy: 0.9731\n",
      "Epoch 17/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9445\n",
      "Epoch 17: val_accuracy did not improve from 0.97605\n",
      "66/66 [==============================] - 16s 238ms/step - loss: 0.1732 - accuracy: 0.9445 - val_loss: 0.1097 - val_accuracy: 0.9760\n",
      "Epoch 18/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9569\n",
      "Epoch 18: val_accuracy did not improve from 0.97605\n",
      "66/66 [==============================] - 16s 239ms/step - loss: 0.1509 - accuracy: 0.9569 - val_loss: 0.1280 - val_accuracy: 0.9701\n",
      "Epoch 19/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9502\n",
      "Epoch 19: val_accuracy did not improve from 0.97605\n",
      "66/66 [==============================] - 16s 238ms/step - loss: 0.1399 - accuracy: 0.9502 - val_loss: 0.1253 - val_accuracy: 0.9701\n",
      "Epoch 20/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9445\n",
      "Epoch 20: val_accuracy improved from 0.97605 to 0.97904, saving model to D:\\NullClass_Internship\\Sleep_Detection_Part3\\Other_format\\sleep_detection_model.h5\n",
      "66/66 [==============================] - 17s 253ms/step - loss: 0.1581 - accuracy: 0.9445 - val_loss: 0.1101 - val_accuracy: 0.9790\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the model\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:16:10.922680600Z",
     "start_time": "2024-06-15T07:15:59.001756400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "import gradio as gr\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "saved_model_dir = os.path.join(base_dir, 'saved_model')\n",
    "\n",
    "# Loading the trained CNN model\n",
    "model = load_model(saved_model_dir)\n",
    "\n",
    "# Initializing the MTCNN face detector\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T22:43:57.772018Z",
     "start_time": "2024-06-14T22:43:57.767019500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making a function for fetching roi coordinates, performing classification and displaying image having detection\n",
    "def classify_faces(img):\n",
    "    faces = detector.detect_faces(img)\n",
    "    sleepy_faces = 0\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, w, h = face['box']\n",
    "        x1 = face['keypoints']['left_eye'][0]\n",
    "        y1 = face['keypoints']['left_eye'][1]\n",
    "        x2 = face['keypoints']['right_eye'][0]\n",
    "        y2 = face['keypoints']['right_eye'][1]\n",
    "\n",
    "        # Calculating the distance between the eyes\n",
    "        eye_distance = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "        if abs(x2 - x1) > abs(y2 - y1):\n",
    "            # For larger horizontal distances between eyes\n",
    "            roi_w = int(5 / 3 * eye_distance)\n",
    "            roi_h = int(2 / 3 * eye_distance)\n",
    "        else:\n",
    "            # For larger vertical distances between eyes\n",
    "            roi_w = int(2 / 3 * eye_distance)\n",
    "            roi_h = int(5 / 3 * eye_distance)\n",
    "\n",
    "        # Calculating the center point between the eyes\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "\n",
    "        # Adjusting ROI coordinates to keep the center point between the eyes (It essentially grabs the top left\n",
    "        # coordinate of the roi box)\n",
    "        roi_x = int(center_x - roi_w / 2)\n",
    "        roi_y = int(center_y - roi_h / 2)\n",
    "\n",
    "        # Ensuring the ROI is within image boundaries\n",
    "        roi_x = max(0, roi_x)\n",
    "        roi_y = max(0, roi_y)\n",
    "        roi_w = min(roi_w, img.shape[1] - roi_x)\n",
    "        roi_h = min(roi_h, img.shape[0] - roi_y)\n",
    "\n",
    "        crop = img[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n",
    "\n",
    "        # Preprocessing the cropped face image as required by your model\n",
    "        crop_resized = cv2.resize(crop, (224, 224))  # Assuming your model expects 224x224 input\n",
    "        crop_resized = crop_resized.astype('float32') / 255.0  # Normalize if required\n",
    "        crop_resized = np.expand_dims(crop_resized, axis=0)  # Add batch dimension\n",
    "\n",
    "        prediction = model.predict(crop_resized)\n",
    "        label = 'Awake' if prediction[0][0] < 0.5 else 'Sleepy'\n",
    "\n",
    "        if label == 'Sleepy':\n",
    "            sleepy_faces += 1\n",
    "            # Drawing bounding box around drowsy face\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            # Putting text label above the bounding box\n",
    "            cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    # Displaying the count of sleepy faces detected\n",
    "    cv2.putText(img, f'Sleepy faces: {sleepy_faces}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    return img, sleepy_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T22:43:58.634912400Z",
     "start_time": "2024-06-14T22:43:58.605126100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Unable to load image from {image_path}\")\n",
    "\n",
    "    # Converting BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resizing the image to fit within a fixed window size while maintaining aspect ratio\n",
    "    max_display_size = 800  # Maximum width or height for displaying the image\n",
    "    height, width, _ = img_rgb.shape\n",
    "    if max(height, width) > max_display_size:\n",
    "        if height > width:\n",
    "            new_height = max_display_size\n",
    "            new_width = int(width * (max_display_size / height))\n",
    "        else:\n",
    "            new_width = max_display_size\n",
    "            new_height = int(height * (max_display_size / width))\n",
    "        img_rgb = cv2.resize(img_rgb, (new_width, new_height))\n",
    "\n",
    "    # Classifying faces and retrieving image with bounding boxes\n",
    "    img_with_boxes, sleepy_faces = classify_faces(img_rgb)\n",
    "\n",
    "    # Converting back to BGR for saving with OpenCV\n",
    "    img_with_boxes_bgr = cv2.cvtColor(img_with_boxes, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return img_with_boxes_bgr, f'Sleepy faces detected: {sleepy_faces}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T22:43:59.451538200Z",
     "start_time": "2024-06-14T22:43:59.428539100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    max_sleepy_faces = 0\n",
    "\n",
    "    # Obtaining frame dimensions and FPS from the video capture\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Converting the frame from BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        frame_with_boxes, sleepy_faces = classify_faces(frame_rgb)\n",
    "        frames.append(frame_with_boxes)\n",
    "\n",
    "        # Updating maximum sleepy faces count if current frame has more\n",
    "        if sleepy_faces > max_sleepy_faces:\n",
    "            max_sleepy_faces = sleepy_faces\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Saving the processed video to a temporary file\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n",
    "    out = cv2.VideoWriter(temp_file.name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    for frame in frames:\n",
    "        # Converting the frame back to BGR for saving\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    return temp_file.name, f'The maximum number of sleepy faces detected in the entire video is: {max_sleepy_faces}'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
